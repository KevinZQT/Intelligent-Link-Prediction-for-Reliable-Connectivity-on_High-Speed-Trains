# Shortâ€‘Term Downlink Throughput Prediction on Trains

> **Thesis companion repository.** This repo contains data collection utilities, feature engineering, and model pipelines to perform **t â†’ t+1** oneâ€‘step prediction of cellular **downlink throughput (Mbps)** at 1â€¯Hz in a real commuterâ€‘rail setting.

The work uses realâ€‘world measurements collected on the Dublin DART line with multiple USB modems (different operators) and phone GPS. We engineer strictlyâ€‘causal features and evaluate several models under a leakageâ€‘safe timeâ€‘ordered split.

---

## âœ¨ Highlights

- **Real data collection pipeline:** Linux SBC + ModemManager (`mmcli`) for radio KPIs; continuous reverseâ€‘direction `iperf3` downlink to measure perâ€‘second throughput; 1â€¯Hz GPS from a phone, with UTC alignment.
- **Leakageâ€‘safe evaluation:** **80/20 chronological split** with a **25â€¯s embargo** before the test window to cover the longest lag (20â€¯s) + safety margin.
- **Causal features:** Current values, short histories (throughput lags up to **20â€¯s**, radio KPI lags up to **3â€¯s**), 5â€‘second rolling statistics (mean/std/min/max), momentum/differences, volatility, and simple state flags (e.g., handover).
- **Model zoo:** Persistence & **AR(1)** baselines; classical ML (**Random Forest**), **ANN/MLP**; sequence model **TCN**; and a **TCN+DQN** hybrid variant.
- **Key finding (from the thesis):** On heldâ€‘out tests for two representative modems, **Random Forest** achieves **RÂ² â‰ˆ 0.53/0.37** with **MAE â‰ˆ 17.1/19.5â€¯Mbps**, clearly outperforming linear AR(1) (negative RÂ², MAE â‰ˆ 35.6/33.2â€¯Mbps).

---

## ğŸ“ Repository Layout

```
.
â”œâ”€ data_collect.py          # Field collection: mmcli + iperf3 (1 Hz throughput & KPIs) â†’ raw CSV
â”œâ”€ data_merge.ipynb         # Align & clean raw modem/GPS/iperf3 streams to a single 1 Hz timeline
â”œâ”€ persistent_ar1.py        # Baselines: Persistence, AR(1); includes a simple RF check; unified metrics
â”œâ”€ rf.py                    # Random Forest (residual target y_{t+1} - y_t) + feature selection + scaling
â”œâ”€ ann.py                   # MLP/ANN with the same engineered features & evaluation protocol
â”œâ”€ tcn.py                   # Temporal Convolutional Network (causal/dilated convs, early stopping)
â”œâ”€ tcn+dqn.ipynb            # Hybrid TCN + DQN experiment (policy/threshold refinement on TCN output)
â””â”€ thesis.pdf               # The dissertation (rename of 9c5cc53e-fffc-44cb-983a-527f2f6d3ad8.pdf)
```

> Tip: you can rename the provided thesis PDF to `thesis.pdf` in the repository.

---

## ğŸ§° Environment

- **Python** 3.10+ (recommended)
- **Core:** `numpy`, `pandas`, `scikit-learn`, `matplotlib`
- **Deep learning:** `tensorflow` (for ANN/TCN)
- **Notebooks:** `jupyter` (for the `*.ipynb` files)
- **Collection (optional, Linux):** `modemmanager` (`mmcli`), `iperf3`

Quick install:

```bash
pip install numpy pandas scikit-learn tensorflow matplotlib jupyter
# optional for collection
# sudo apt-get install modemmanager iperf3
```

---

## ğŸ“¦ Data Expectations

Most training scripts expect a **processed** CSV at the repo root, named **`data_filled.csv`**, with at least:

- `timestamp` (UTC), `modem_id`, `downlink_mbps`
- If available: `rssi, rsrp, rsrq, snr, speed_gps, cell_id, ...`

The collection script `data_collect.py` writes raw CSVs (default `modem_downlink_only.csv`) from `mmcli` and `iperf3`. Use `data_merge.ipynb` to **align** and **clean** the raw streams into the 1â€¯Hz `data_filled.csv` used by the models.

---

## ğŸš€ Quick Start (with processed data)

1. **Place** your processed **`data_filled.csv`** in the repository root.
2. **Run** any of the following (each trains & evaluates **perâ€‘modem**, using the same split/embargo and metrics):

```bash
# Baselines: Persistence + AR(1) + a simple RF sanity check
python persistent_ar1.py

# Random Forest (thesis best overall)
python rf.py

# MLP / ANN (same engineered features and protocol)
python ann.py

# Temporal Convolutional Network (causal/dilated)
python tcn.py
```

Each script prints a perâ€‘modem summary (RÂ², MAE, MAPE vs current, and useful distributional stats).

---

## ğŸ§ª Reproducibility & Evaluation Protocol

- **Objective:** oneâ€‘step forecast **`y_{t+1}`** (nextâ€‘second downlink Mbps). Many scripts learn the **residual** `y_{t+1} - y_t` and add back `y_t` at inference.
- **Split:** timeâ€‘ordered; last **20%** as **test**, with a **25â€¯s embargo** immediately before test to prevent lookâ€‘ahead via lagged features.
- **Adjacency constraint:** evaluate only pairs with **Î”t â‰¤ 1.1â€¯s** (tightly 1â€¯Hz).
- **Metrics:** MAE (Mbps), RÂ², and percentage errors w.r.t. current rate (MAPEâ€‘style; treat nearâ€‘zero throughput with caution).

These rules are encoded in the training scripts; no extra config needed for a standard run.

---

## âš™ï¸ Configuration (key defaults)

- **Feature lags:** throughput lags **1..20â€¯s**, radioâ€‘KPI lags **1..3â€¯s**.
- **Rolling stats:** window **5â€¯s** (mean/std/min/max).
- **Feature selection:** `SelectKBest(f_regression)` up to **30** features; `RobustScaler` for stability.
- **Split & safety:** `test_ratio=0.2`, **25â€¯s embargo**, adjacency **â‰¤ 1.1â€¯s**.
- **Training (ANN/TCN):** early stopping (`patience=15`), `ReduceLROnPlateau`, small minâ€‘LR, light hyperâ€‘param tuning toggles.
- **Percentage error floor:** 0.1â€¯Mbps to avoid division blowâ€‘ups at ~0 throughput.

Scriptâ€‘specific notes:

- `ann.py` reads `CFG.input_csv` (default **`data_filled.csv`**).
- `rf.py` and `persistent_ar1.py` read **`data_filled.csv`** directly.
- `tcn.py` accepts `--data PATH` (defaults to a CSV path); it builds features internally before training.

---

## ğŸ“Š Key Results (from the thesis)

On two representative modems in the heldâ€‘out test split:

| Model             | RÂ² (M1)  | MAE (Mbps, M1) | RÂ² (M2)  | MAE (Mbps, M2) |
| ----------------- | :------: | :------------: | :------: | :------------: |
| Persistence       |    â€”     |       â€”        |    â€”     |       â€”        |
| AR(1)             |  -0.49   |      35.6      |  -0.50   |      33.2      |
| **Random Forest** | **0.53** |    **17.1**    | **0.37** |    **19.5**    |
| ANN / MLP         |  ~0.51   |     ~18â€“20     |  ~0.21   |     ~23â€“24     |
| TCN               |  ~0.23   |     ~24â€“25     |  ~0.10   |     ~25â€“26     |
| TCN + DQN         |  ~0.19   |     ~25â€“30     |  ~0.11   |      ~30       |

> RF consistently achieves the best accuracyâ€“robustness tradeâ€‘off in this smallâ€‘sample, rapidly varying setting. See **`thesis.pdf`** for complete tables (incl. MAPE and distributional quantiles).

---

## ğŸ§± (Optional) Data Collection Workflow

1. **Hardware & setup:** Linux SBC + 3Ã— USB cellular modems (multiple operators).
2. **KPIs:** use `mmcli` to poll RSRP/RSRQ/RSSI/SNR, EARFCN, RAT, MCC/MNC, TAC, Cell ID, etc., at 1â€¯Hz.
3. **Throughput:** run `iperf3` in **reverse** (server â†’ client) downlink mode; parse perâ€‘second rates.
4. **GPS:** 1â€¯Hz phone GPS; ensure UTC synchronization with the SBC.
5. **Merge:** align modem/GPS/iperf3 streams to a single 1â€¯Hz axis; carefully handle gaps/handovers (no crossâ€‘segment forwardâ€‘fill); clamp outliers (e.g., [0, 200]â€¯Mbps).

`data_collect.py` shows a minimal reference implementation (concurrency for `iperf3`, `mmcli` parsing, CSV writing).

---

## ğŸ” Reâ€‘running the Thesis Experiments

- Use `data_merge.ipynb` to reproduce the **processed** dataset used in the thesis (`data_filled.csv`).
- Then run `rf.py`, `ann.py`, and `tcn.py` as above.
- For the hybrid experiment, open `tcn+dqn.ipynb` and follow the notebook cells.

---

## ğŸ“„ License

Choose a license that fits your needs (e.g., **MIT** or **Apacheâ€‘2.0**). Add a `LICENSE` file at the repo root.

---

## ğŸ“ Citation

If this repository or the associated thesis is useful to your research, please cite it. Replace placeholders below with your final bibliographic information.

```
@thesis{<your-key>,
  title   = {Short-Term Downlink Throughput Prediction on Trains},
  author  = {<Your Name>},
  school  = {<Your University/Department>},
  year    = {2025},
  note    = {Code and data processing pipelines available at <your GitHub URL>}
}
```

---

## ğŸ™ Acknowledgements

Thanks to everyone who helped with data collection and to the network operators whose infrastructure made this study possible.
